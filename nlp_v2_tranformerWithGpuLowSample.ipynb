{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install \n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "pip install transformers datasets scikit-learn matplotlib seaborn tqdm pandas\n",
    "\n",
    "install PyTorch with CUDA support\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "Install other libraries\n",
    "pip install transformers datasets scikit-learn matplotlib seaborn tqdm pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")  # Print if using CUDA (GPU) or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ataislucky/Data-Science/main/dataset/emotion_train.txt\"\n",
    "\n",
    "# Load the dataset, assuming it's a text file with each row in the format: text;label\n",
    "df = pd.read_csv(url, delimiter=';', header=None, names=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='label', data=df, palette='viridis')\n",
    "plt.title('Emotion Distribution')\n",
    "plt.xlabel('Emotion Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text'].tolist()\n",
    "y = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding to numeric for BERT (if not already done)\n",
    "unique_labels = sorted(set(y))\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "y = [label2id[label] for label in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for text classification\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text using the tokenizer provided by the transformers library\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer and model for sequence classification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(unique_labels)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross-Validation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "accuracy_scores = []\n",
    "classification_reports = []\n",
    "confusion_matrices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "\n",
    "    # Create PyTorch Datasets\n",
    "    train_dataset = TextClassificationDataset(X_train, y_train, tokenizer, max_len=128)\n",
    "    test_dataset = TextClassificationDataset(X_test, y_test, tokenizer, max_len=128)\n",
    "\n",
    "    # Create DataLoader for batching\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "    # Optimizer and learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    num_training_steps = len(train_loader) * 3  # 3 epochs\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(3):  # 3 epochs\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": running_loss / len(train_loader)})\n",
    "\n",
    "        print(\"epoch : \",epoch)\n",
    "    print(\"end KFold\")\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "            y_true.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy, classification report, and confusion matrix\n",
    "    accuracy_scores.append(accuracy_score(y_true, y_pred))\n",
    "    classification_reports.append(classification_report(y_true, y_pred, target_names=unique_labels, output_dict=True))\n",
    "    confusion_matrices.append(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average accuracy\n",
    "avg_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "print(f\"\\nAverage Accuracy across folds: {avg_accuracy:.4f}\")\n",
    "\n",
    "# Print classification reports and confusion matrices for each fold\n",
    "for i, report in enumerate(classification_reports):\n",
    "    print(f\"\\nClassification Report for Fold {i+1}:\\n\")\n",
    "    print(pd.DataFrame(report).transpose())\n",
    "\n",
    "for i, matrix in enumerate(confusion_matrices):\n",
    "    print(f\"\\nConfusion Matrix for Fold {i+1}:\\n\")\n",
    "    print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
